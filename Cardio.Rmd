---
title: "Cardio"
author: "Daniel Herrera"
date: "11/14/2021"
output: html_document
---
Cardiovascular diseases kill approximately 17 million people globally every year, and they mainly exhibit as myocardial infarctions and heart failures. Heart failure occurs when the heart cannot pump enough blood to meet the needs of the body. Available electronic medical records of patients quantify symptoms, body features, and clinical laboratory test values, which can be used to perform biostatistics analysis aimed at highlighting patterns and correlations otherwise undetectable by medical doctors. Machine learning, in particular, can predict patients’ survival from their data and can individuate the most important features among those included in their medical records.

We will be exlploring data from this [study](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5#Tab4) that includes 299 patients with heart failure. The variables available are described below.

- `age`: age of the patient (years)
- `anaemia`: decrease of red blood cells or hemoglobin (binary; 0 = no, 1 = yes)
- `high_blood_pressure`: if the patient has hypertension (binary; 0 = no, 1 = yes)
- `creatinine_phosphokinase` (CPK): level of the CPK enzyme in the blood (mcg/L)
- `diabetes`: if the patient has diabetes (binary; 0 = no, 1 = yes)
- `ejection_fraction`: percentage of blood leaving the heart at each contraction (percentage)
- `platelets`: platelets in the blood (kiloplatelets/mL)
- `sex`: woman or man (binary; 0 = female, 1 = male)
- `serum_creatinine`: level of serum creatinine in the blood (mg/dL)
- `serum_sodium`: level of serum sodium in the blood (mEq/L)
- `smoking`: if the patient smokes or not (binary; 0 = no, 1 = yes)
- `time`: follow-up period (days)
- `DEATH_EVENT`: if the patient deceased during the follow-up period (binary; 0 = no, 1 = yes)


Here we will build models to predict the outcome, `DEATH_EVENT`. Run the code below to load the data and create training and test sets. We will use a 70% training, 30% test set split. Note that we have an imbalance in outcome groups (96/299 = 32% of patients died and 68% did not), so we need to take this into account when we split our data into training and test sets. Here we use a new function, `stratified`, from the `splitstackshape` package to split the data. The `stratified` function samples the same percent of individuals from each class - in this case, `DEATH_EVENT`.  

```{r}
# library upload
library(splitstackshape)
library(tidyverse)
library(caret)
library(MASS)
library(pROC)
library(rpart)
library(randomForest)
library(e1071)
library(knitr)
library(ggthemes)

data <- read.csv("heart_failure_clinical_records_dataset.csv")

set.seed(1)

# use stratified to keep the same percent of individuals in outcomes when sampling
x <- stratified(data, "DEATH_EVENT", 0.7, keep.rownames = TRUE)
# not quite sure what this is doing? 
train_set <- x %>% dplyr::select(-rn)
# the training index is equal to rn values
train_index <- as.numeric(x$rn)
# the test set is all others
test_set <- data[-train_index,] 

dim(train_set)
dim(test_set)
```


The accuracy of the logistic regression model is better than a random coin toss (at 68.89%). We see that the sensitivity, our ability to correctly predict true positives (predicting death) among those who did in fact die, is quite low (41.38%) However, this is imbalanced with the  relatively high specificity of 81.97%. 
Thus, among those who survive (death event = 0), we are not predicting many correctly as surviving. Our model is thereby making more errors in correctly identifying those who survive (death event = 0) than those who die (death event = 1);  this is further shown by the higher value for the Negative Predictive Value (NPV) compared to the Positive Predictive Value (PPV) than.


```{r}
# build model to the training set 
glm_full_model <- glm(DEATH_EVENT ~ .-time,  family = "binomial", data = train_set)
summary(glm_full_model) # use summary to see that time is removed

# predict function 
p_hat_glm <- predict(glm_full_model, newdata = test_set, type = "response")
mypred_glm <- ifelse(p_hat_glm > 0.5, 1, 0)

# make confusion matrix
confusionMatrix(data = as.factor(mypred_glm), reference = as.factor(test_set$DEATH_EVENT), positive = "1")

```


The accuracy of the naive Bayes model is better than a random coin toss (at 66.67%), but it is lower than the logistic regression model. We see that the sensitivity, our ability to correctly predict true positives (predicting death) among those who did in fact die, is quite low at 24.14%%. However, this is imbalanced with the a relatively high specificity of 86.89%. Thus, among those who actually survive (death event = 0), we are predicting many correctly as surviving. Our model is thereby making more errors in correctly identifying those who die (death event = 1) than those who survive (death event = 0); this is further shown by the lower value for the Positive Predictive Value (PPV) than the Negative Predictive Value (NPV).

```{r}
# create full model using naiveBayes without time as covariate
nb_full_model <- naiveBayes(DEATH_EVENT ~ .-time, data = train_set)
#create yhat predictions, here they will be predictions and not probabilities
mypred_nb <- predict(nb_full_model, test_set)
confusionMatrix(data = as.factor(mypred_nb), reference = as.factor(test_set$DEATH_EVENT), positive = "1")

```


The accuracy of the k-nearest neighbors model is better than a random coin toss (at 71.11%) and is the highest of our previous models. We see that the specificity, our ability to correctly predict true negatives (survivors) among those who did in fact survive, is quite high at 98.36%. However, this is imbalanced with the a very low sensitivity of 13.79%. Thus, among those who actually die (death event = 1), we are not predicting many correctly as dying, and in fact doing far worse than a random coin flip. Our model is thereby making more errors in correctly identifying those who die (death event = 1) than those who survive (death event = 0).



```{r}
# knn using all but time as covariates, and k = 12 neighbors
# cutoff 0.5 for predictors
knn_model <- knn3(DEATH_EVENT ~ .-time, data = train_set, k = 12) # chose 12 nearest neighbors
p_hat_knn <- predict(knn_model, newdata = test_set)[,2] # chose column 2 because this is prob y=1
mypred_knn <- ifelse(p_hat_knn > 0.5, 1, 0)

confusionMatrix(data = as.factor(mypred_knn), reference = as.factor(test_set$DEATH_EVENT), positive = "1")
```


There is an increase in terms of overall accuracy for all models, which is shown in the table below. 
In terms of overall accuracy, the best performing model is still the KNN model. 

```{r}
## Logistic regression
# build model to the training set 
glm_red_model <- glm(DEATH_EVENT ~ ejection_fraction + serum_creatinine,  family = "binomial", data = train_set)
summary(glm_red_model) # use summary to see that time is removed
# predict function 
p_hat_glm <- predict(glm_red_model, newdata = test_set, type = "response")
mypred_glm <- ifelse(p_hat_glm > 0.5, 1, 0)
# make confusion matrix
confusionMatrix(data = as.factor(mypred_glm), reference = as.factor(test_set$DEATH_EVENT), positive = "1")



## Naive Bayes
nb_red_model <- naiveBayes(DEATH_EVENT ~ ejection_fraction + serum_creatinine, data = train_set)
#create yhat predictions, here they will be predictions and not probabilities
mypred_nb <- predict(nb_red_model, test_set)
confusionMatrix(data = as.factor(mypred_nb), reference = as.factor(test_set$DEATH_EVENT), positive = "1")


## kNN
knn_red_model <- knn3(DEATH_EVENT ~ ejection_fraction + serum_creatinine, data = train_set, k = 12) # chose 12 nearest neighbors
p_hat_knn <- predict(knn_red_model, newdata = test_set)[,2] # chose column 2 because this is prob y=1
mypred_knn <- ifelse(p_hat_knn > 0.5, 1, 0)

confusionMatrix(data = as.factor(mypred_knn), reference = as.factor(test_set$DEATH_EVENT), positive = "1")



# table of accuracy across models
accuracies <- data.frame(model = c("Logistic", "NB", "KNN"),
                         accuracy = c(
                           confusionMatrix(data = as.factor(mypred_glm), reference = as.factor(test_set$DEATH_EVENT))$overall[1],
                           confusionMatrix(data = as.factor(mypred_nb), reference = as.factor(test_set$DEATH_EVENT))$overall[1],
                           confusionMatrix(data = as.factor(mypred_knn), reference = as.factor(test_set$DEATH_EVENT))$overall[1]),
                         previousAccuracy = c(0.6889, 0.6667, 0.7111)
)
accuracies 
```


We will plot ROC curves to visualize the performance of these models. 

```{r, message=FALSE}
glm_roc <- roc(test_set$DEATH_EVENT, p_hat_glm)

knn_roc <- roc(test_set$DEATH_EVENT, p_hat_knn)

mypred_raw_nb <- predict(nb_red_model, test_set, type = "raw")[,2]
nb_roc <- roc(test_set$DEATH_EVENT, mypred_raw_nb)

roclist <- list("Logistic" = glm_roc, 
                "KNN(k=12)" = knn_roc,
                "Naive Bayes" = nb_roc)

# note legacy.axes makes it go from 0 to 1
g2 <- ggroc(roclist, legacy.axes = T, aes = "colour") +
  geom_abline(linetype = "dashed", alpha = 0.4) + 
  ggtitle("ROC Curve") +
  xlab("1- Specificity") + 
  ylab("Sensitivity")+
  guides(colour = guide_legend(title = "Models")) + 
  theme_wsj() +
  theme(title = element_text(size = 15)) +
  theme(axis.title = element_text(size = 12, face = "bold")) +
  theme(legend.title = element_text(size = 10, face = "bold"))

g2
```

The highest AUC is achieved by the K nearest neighbors model (KNN). We can see how this is reflected in the ROC Curve as well which indicates it is the better model as it shows a higher sensitivity almost across the range of specificity values along the ROC Curve. 

Additionally, we saw that the highest accuracy was achieved by the KNN model, so that provides another reason to select KNN as the preferred model. 

Lastly, the sensitivity for the KNN model is as high or higher than the other models. If we are more interested in our predictions of among those with the event of death, then this model is our preferred choice. 

```{r}
# calculate AUC
roclist <- list("Logistic" = glm_roc, 
                "KNN(k=12)" = knn_roc,
                "Naive Bayes" = nb_roc)

kable(data.frame(lapply(roclist,auc)))
```




```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(splitstackshape)
library(caret)
library(MASS)
library(pROC)
library(rpart)
library(randomForest)
library(knitr)

# library add ggthemes and install.packages
# install.packages("ggthemes", repos = "http://cran.us.r-project.org")
library(ggthemes)
```

### Heart Failure Revisited

We will use the same cardiovascular dataset from HW3. As a reminder, cardiovascular diseases kill approximately 17 million people globally every year, and they mainly exhibit as myocardial infarctions and heart failures. Heart failure occurs when the heart cannot pump enough blood to meet the needs of the body. Available electronic medical records of patients quantify symptoms, body features, and clinical laboratory test values, which can be used to perform biostatistics analysis aimed at highlighting patterns and correlations otherwise undetectable by medical doctors. Machine learning, in particular, can predict patients’ survival from their data and can individuate the most important features among those included in their medical records.

We will be using data from this [study](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5#Tab4) that includes 299 patients with heart failure. The variables available are described below.

- `age`: age of the patient (years)
- `anaemia`: decrease of red blood cells or hemoglobin (binary; 0 = no, 1 = yes)
- `high_blood_pressure`: if the patient has hypertension (binary; 0 = no, 1 = yes)
- `creatinine_phosphokinase` (CPK): level of the CPK enzyme in the blood (mcg/L)
- `diabetes`: if the patient has diabetes (binary; 0 = no, 1 = yes)
- `ejection_fraction`: percentage of blood leaving the heart at each contraction (percentage)
- `platelets`: platelets in the blood (kiloplatelets/mL)
- `sex`: woman or man (binary; 0 = female, 1 = male)
- `serum_creatinine`: level of serum creatinine in the blood (mg/dL)
- `serum_sodium`: level of serum sodium in the blood (mEq/L)
- `smoking`: if the patient smokes or not (binary; 0 = no, 1 = yes)
- `time`: follow-up period (days)
- `DEATH_EVENT`: if the patient deceased during the follow-up period (binary; 0 = no, 1 = yes)


Here we will build models to predict the outcome, `DEATH_EVENT`. Run the code below to load the data and create training and test sets. We will use a 70% training, 30% test set split. Note that we have an imbalance in outcome groups (96/299 = 32% of patients died and 68% did not), so we need to take this into account when we split our data into training and test sets. Here we use a new function, `stratified`, from the `splitstackshape` package to split the data. The `stratified` function samples the same percent of individuals from each class - in this case, `DEATH_EVENT`. Note: keep `set.seed(1)` so you get the same train/test split and model predictions we do. We have also made all binary and categorical variables into factors here so you do not need to do it later on or in your models.

```{r}
data <- read.csv("heart_failure_clinical_records_dataset.csv")

# Turn categorical variables into factors (and not numeric)
data <- data %>% mutate(diabetes = as.factor(diabetes),
                        sex = as.factor(sex),
                        anaemia = as.factor(anaemia),
                        high_blood_pressure = as.factor(high_blood_pressure),
                        smoking = as.factor(smoking),
                        DEATH_EVENT = as.factor(DEATH_EVENT))

set.seed(1)

x <- stratified(data, "DEATH_EVENT", 0.7, keep.rownames = TRUE)
train_set <- x %>% dplyr::select(-rn)
train_index <- as.numeric(x$rn)
test_set <- data[-train_index,]

dim(train_set)
dim(test_set)
```


### QDA, LDA, Decision Trees and Random Forests

We see that the accuracy of our model is 72.22% for the LDA model. The sensitivity and specificity are imbalanced; this suggests in this case that our model is better able to predict those who do not actually die (survive) compared to correctly predicting those who die, indicated by our lower sensitivity relative to specificity. 


```{r}
set.seed(1)
# use lda on training sit to predict death_event
# all variables except time
lda_model <- lda(DEATH_EVENT ~ .-time, data = train_set)

# use predict to get estimates for test set
# this will give predicted probabilities
lda_prob <- predict(lda_model, test_set)$posterior[,2]

# use 0.5 as the cutoff for probabilities 
lda_pred <- ifelse(lda_prob >= 0.5, 1, 0)

# obtain accuracy, confusion matrix
confusionMatrix(data = as.factor(lda_pred), reference = as.factor(test_set$DEATH_EVENT), positive = "1")
acc_lda <- confusionMatrix(data = as.factor(lda_pred), reference = as.factor(test_set$DEATH_EVENT), positive = "1")$overall[1]


```



The accuracy for the QDA model is 71.11%, lower than LDA.We see that similar to LDA, the sensitivity and specificity are imbalanced; this suggests in this case that our model is better able to predict those who do not actually die (survive) compared to correctly predicting those who die, indicated by our lower sensitivity relative to specificity. 


```{r}
set.seed(1)
# use qda on training sit to predict death_event
# all variables except time
qda_model <- qda(DEATH_EVENT ~ .-time, data = train_set)

# use predict to get estimates for test set
# this will give predicted probabilities
qda_prob <- predict(qda_model, test_set)$posterior[,2]

# use 0.5 as the cutoff for probabilities 
qda_pred <- ifelse(qda_prob >= 0.5, 1, 0)

# obtain accuracy, confusion matrix
confusionMatrix(data = as.factor(qda_pred), reference = as.factor(test_set$DEATH_EVENT), positive = "1")
acc_qda <- confusionMatrix(data = as.factor(qda_pred), reference = as.factor(test_set$DEATH_EVENT), positive = "1")$overall[1]

```



We see here that the accuracy is the highest of all previous models at 74.44%. We continue to notice an imbalance in sensitivity and specificity; we are continuing to have a more challenging time predicting those who die out of those whom actually died when compared to correctly predicting survivors of those who survived, indicated  as our sensitivity is lower than our specificity. 


```{r}
set.seed(1)
# fit a decision tree using rpart
# all predictors except time 

tree_model <- rpart(DEATH_EVENT ~ .-time, data = train_set)

# add stuff from lecture recording here
# probabilities here 
tree_probs <- predict(tree_model, newdata = test_set)[,2]
tree_preds <- factor(ifelse(tree_probs >= 0.5, 1, 0))

# obtain confustionMatrix
confusionMatrix(data = tree_preds, reference = test_set$DEATH_EVENT, positive = "1")

acc_tree <- confusionMatrix(data = tree_preds, reference = test_set$DEATH_EVENT, positive = "1")$overall[1]

```




The accuracy in our random forest model is the same as the decision tree at 74.44%. As before, we continue to see the same pattern with sensitivity and specificity as mentioned previously. 


```{r}
set.seed(1)
# fit a randomForest model
# all predictors except time 
# we will use default randomforest arguments
forest_model <- randomForest(DEATH_EVENT ~ .-time, data = train_set)


# probabilities predicted
forest_probs <- predict(forest_model, newdata = test_set, type = "prob")[,2]
# binary predictions
forest_preds <- factor(ifelse(forest_probs > 0.5, 1, 0))

# obtain confusionMatrix
confusionMatrix(data = as.factor(forest_preds), reference = test_set$DEATH_EVENT, positive = "1")
acc_forest <- confusionMatrix(data = forest_preds, reference = test_set$DEATH_EVENT, positive = "1")$overall[1]

```



Yes, our top two predictors are the most predictive features, ie highest Gini index; this coincides with the authors findings. 

```{r}
# calculate cariable importance using Gini index 
variable_importance <- importance(forest_model) 

tmp <- data_frame(feature = rownames(variable_importance),
                  Gini = variable_importance[,1]) %>%
                  arrange(desc(Gini)) # arrange from highest to lowest gini value
kable(tmp[1:2,])

```




We see that the accuracy does increase for LDA from model 1 to model 2 (ejection fraction and serum creatinine only) from 72.22 to 73.33.

We see that the accuracy of our decision tree model increases to 80% from 74.44% when we use only the two most important predictors. 

For our qda model and our random forest model, we notice that there is no change from one model to the next in accuracy. 


```{r}
set.seed(1)

# LDA
lda_mod2 <- lda(DEATH_EVENT ~ ejection_fraction + serum_creatinine, data = train_set)
lda_probs2 <- predict(lda_mod2, test_set)$posterior[,2]
lda_pred2 <- ifelse(lda_probs2 >= 0.5, 1, 0)
confusionMatrix(data = as.factor(lda_pred2), reference = as.factor(test_set$DEATH_EVENT), positive = "1")

acc_lda2 <- confusionMatrix(data = as.factor(lda_pred2), reference = as.factor(test_set$DEATH_EVENT), positive = "1")$overall[1]

# QDA
qda_mod2 <- qda(DEATH_EVENT ~ ejection_fraction + serum_creatinine, data = train_set)
qda_probs2 <- predict(qda_mod2, test_set)$posterior[,2]
qda_pred2 <- ifelse(qda_probs2 >= 0.5, 1, 0)
confusionMatrix(data = as.factor(qda_pred2), reference = as.factor(test_set$DEATH_EVENT), positive = "1")

acc_qda2 <- confusionMatrix(data = as.factor(qda_pred2), reference = as.factor(test_set$DEATH_EVENT), positive = "1")$overall[1]

# Decision Tree

tree_model2 <- rpart(DEATH_EVENT ~ ejection_fraction + serum_creatinine, data = train_set)
tree_probs2 <- predict(tree_model2, newdata = test_set)[,2]
tree_preds2 <- factor(ifelse(tree_probs2 >= 0.5, 1, 0))
confusionMatrix(data = tree_preds2, reference = test_set$DEATH_EVENT, positive = "1")

acc_tree2 <- confusionMatrix(data = tree_preds2, reference = test_set$DEATH_EVENT, positive = "1")$overall[1]


# Random Forest
forest_model2 <- randomForest(DEATH_EVENT ~ ejection_fraction + serum_creatinine, data = train_set)
forest_probs2 <- predict(forest_model2, newdata = test_set, type = "prob")[,2]
forest_preds2 <- factor(ifelse(forest_probs2 > 0.5, 1, 0))
confusionMatrix(data = forest_preds2, reference = test_set$DEATH_EVENT, positive = "1")

acc_forest2 <- confusionMatrix(data = forest_preds2, reference = test_set$DEATH_EVENT, positive = "1")$overall[1]


myaccur <- data.frame(model = c("lda", "qda", "tree", "forest"),
                      fullmod_accuracy = c(acc_lda, acc_qda, acc_tree, acc_forest),
                      reducedmod_accuracy = c(acc_lda2, acc_qda2, acc_tree2, acc_forest2)
                      )

kable(myaccur)

```




```{r, message=FALSE}

# use ggroc here, make a list of the models

lda_roc <- roc(test_set$DEATH_EVENT, lda_probs2)
qda_roc <- roc(test_set$DEATH_EVENT, qda_probs2)
dt_roc <- roc(test_set$DEATH_EVENT, tree_probs2)
rf_roc <- roc(test_set$DEATH_EVENT, forest_probs2)




roclist <- list("LDA" = lda_roc, 
                "QDA" = qda_roc,
                "Decision Tree" = dt_roc,
                "Random Forest" = rf_roc
)

# note legacy.axes makes it go from 0 to 1
g2 <- ggroc(roclist, legacy.axes = T, aes = "colour", size =1.5) +
  geom_abline(linetype = "dashed", alpha = 0.4) + 
  ggtitle("ROC Curve") +
  xlab("1- Specificity") + 
  ylab("Sensitivity")+
  guides(colour = guide_legend(title = "Models")) + 
  theme_wsj() +
  theme(title = element_text(size = 15)) +
  theme(axis.title = element_text(size = 12, face = "bold")) +
  theme(legend.title = element_text(size = 10, face = "bold"))

g2
```




The highest AUC comes from the randomForest model. This model has an AUC of 0.7761. However, the accuracy is highest in the decision tree model. As a result, I would select the decision tree model as the best model, even if it has a lower AUC. Additionally, we see that at a specific cutoff, the decision tree model is actually closest to the point (0,1) on the ROC curve, which is ideal. Lastly, the decision tree offers a more interpretable response than the random forest approach.

```{r}
# calculate AUC 

myrocs <- list("lda" = lda_roc, "qda" = qda_roc, "tree" = dt_roc, "forest" = rf_roc)
kable(data.frame(lapply(myrocs, auc)))

```


